# KIT Transformer
### 3.3 포지션-와이즈 피드 포워드 신경망
트랜스포머의 인코더와 디코더엔 어텐션층 뿐만 아니라 피드 포워드 완전연결층을 포함하며 각 위치에서 개별적이지만 동일하게 동작합니다. 
피드 포워드 완전연결층은 두 개의 선형 변환으로 구성되며 ReLU 활성화 함수를 사용합니다.

```
FFN(x) = max(0, xW1 + b1)W2 + b2
```
선형 변환은 층 내부에서는 동일하게 수행되지만 층마다는 다른 파라미터를 사용합니다. 
이는 커널 크기가 1인 두 개의 컨볼루션과 비슷한 효과를 냅니다.
입력과 출력의 차원(d_model)은 512이며 피드 포워드 신경망의 차원(d_ff)은 2048입니다.


### 3.4 임베딩과 소프트맥스
다른 시퀀스 변환 모델과 유사하게 입력 토큰과 출력 토큰을 d_model 차원의 벡터로 변환시키기 위해 학습된 임베딩을 사용했습니다.
또한 디코더의 출력을 예측된 다음 토큰 확률로 변환시키기 위해 학습된 선형 변환과 소프트맥스 함수를 사용했습니다.
트랜스포머는 사전 소프트맥스 선형 변환과 두 개의 임베딩층 사이에 동일한 가중치 행렬을 공유합니다.
임베딩층에서는 해당 가중치를 √d_model과 곱합니다.

### 3.5 포지셔널 인코딩
트랜스포머는 회귀 혹은 컨볼루션을 사용하지 않기 때문에 모델이 시퀀스 순서 정보를 이용하려면 시퀀스 내부 토큰들의 상대적 혹은 절대적 위치 정보를 주입해줘야 합니다.
이를 위해 트랜스포머는 인코더와 디코더 스택에 입력되기 전 "포지셔널 인코딩" 벡터를 입력 임베딩 벡터와 더하는 과정을 거칩니다.
포지셔널 인코딩 벡터는 d_model 차원이므로 두 벡터를 더할 수 있습니다.
포지셔널 인코딩은 학습된 것과 고정된 것이 있습니다.[9]


트랜스포머에서는 다른 파형을 지닌 사인과 코사인 함수를 사용합니다.

```
PE(pos,2i) = sin(pos/10000**(2*i/dmodel))
PE(pos,2i+1) = cos(pos/10000**(2i/dmodel))
```

위 식에서 pos는 행렬의 행 인덱스를 나타내고, i는 열 인덱스를 나타냅니다. 
즉 포지셔널 인코딩의 각 원소는 사인/코사인 함수의 값에 해당합니다. 
파장은 2π에서 10000 * 2π까지의 기하학적 파형을 형성합니다.
우리는 모델이 상대적인 위치로 인해 더 잘 학습할 것이라고 가정하였고 따라서 고정된 오프셋 k에 대해 PE_pos + k를 PE_pos의 선형 함수로 표현시킬 수 있는 사인과 코사인 함수를 채택했습니다.

또한 학습된 포지셔널 임베딩[9]을 대신 사용하여 실험한 결과 결과에 큰 차이가 없음을 밝혀냈습니다. (표3 E행 참조) 그럼에도 우리가 사인과 코사인 함수를 사용한 이유는 학습된 시퀀스 길이보다 더 긴 시퀀스에 대해 더욱 유연하게 대처할 수 있기 때문입니다.